{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcbf8150-79d6-4f7f-803f-5aaf671466ab",
   "metadata": {},
   "source": [
    "# Вывод основываясь на предпочтениях"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51d79b89-98b8-4410-8a4e-f41a179ac402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Используется обученная модель, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c3cb29d-1786-4ade-afd9-36d044edd9d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               video_id  \\\n",
      "0  2231d493-6837-4f09-a766-b847867bc3d6   \n",
      "1  048a1077-edc2-4f09-bdb9-08c8f461523f   \n",
      "2  4e972100-6a11-4dc2-9146-bb955526af46   \n",
      "3  957005c2-a259-45e1-9685-74589887960d   \n",
      "4  1c1e1973-f492-4361-ad63-4942d2b6fd68   \n",
      "5  6355965d-ae67-419a-b8c8-d088d1eaecdb   \n",
      "6  e5fa6850-a969-4fc9-ae57-8f542b8fd196   \n",
      "7  fd95ff16-da0f-491f-ba74-bee6dc7bb31e   \n",
      "8  20618a8d-3bd9-4a30-8b32-eabf3681b7fb   \n",
      "9  b3181c61-2d66-4114-b0ad-e891c08e20c5   \n",
      "\n",
      "                                               title  \\\n",
      "0            Ирина Хакамада показала свой автомобиль   \n",
      "1                 Друзья / Friends – 5 сезон 2 серия   \n",
      "2  мы вызвали сиреноголового в лесу / переписка Д...   \n",
      "3  «ДНК»: «Отец без документов» | Выпуск от 1 фев...   \n",
      "4  Человек-паук: Возвращение домой | Spider-Man: ...   \n",
      "5                    Красивые девушки в нижнем белье   \n",
      "6                                                  1   \n",
      "7  Женский стендап: Оля Малащенко - Агрессивный ф...   \n",
      "8                     10 глупых вопросов АВИАТЕХНИКУ   \n",
      "9  НОВЫЕ ФИЛЬМЫ 2024, КОТОРЫЕ УЖЕ ВЫШЛИ В ХОРОШЕМ...   \n",
      "\n",
      "             v_pub_datetime   category_id  \n",
      "0 2010-01-01 00:00:01+03:00   Развлечения  \n",
      "1 2023-11-05 16:11:24+03:00       Сериалы  \n",
      "2 2022-05-16 11:31:17+03:00   Развлечения  \n",
      "3 2024-02-01 20:05:08+03:00  Телепередачи  \n",
      "4 2024-01-05 16:43:23+03:00        Фильмы  \n",
      "5 2024-05-10 10:15:48+03:00   Развлечения  \n",
      "6 2010-01-01 00:00:01+03:00        Разное  \n",
      "7 2024-06-26 11:00:07+03:00          Юмор  \n",
      "8 2024-03-28 15:00:32+03:00      Интервью  \n",
      "9 2024-07-06 12:00:07+03:00        Фильмы  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Загрузка модели\n",
    "model = joblib.load('random_forest_model.joblib')\n",
    "\n",
    "# Загрузка данных из файлов\n",
    "filtred_df = pd.read_parquet('tags-filtered.parquet')\n",
    "logs_df = pd.read_parquet('logsV02.parquet')\n",
    "tags_df = pd.read_parquet('users/user0_tags.parquet')\n",
    "regions_df = pd.read_parquet('users/user0_regions.parquet')\n",
    "city_df = pd.read_parquet('users/user0_city.parquet')\n",
    "additional_df = pd.read_parquet('parquet-filtered/filtred.parquet')\n",
    "\n",
    "# Объединение данных по video_id\n",
    "merged_df = pd.merge(filtred_df, logs_df, on='video_id', how='inner')\n",
    "\n",
    "# Ограничиваемся первыми 1000 записями\n",
    "merged_df = merged_df.head(1000)\n",
    "\n",
    "# Признаки\n",
    "features = ['category_id', 'v_likes', 'v_dislikes',\n",
    "            'v_frac_avg_watchtime_1_day_duration',\n",
    "            'v_frac_avg_watchtime_7_day_duration',\n",
    "            'v_frac_avg_watchtime_30_day_duration']\n",
    "\n",
    "# Преобразование категориальных переменных для новых данных\n",
    "X_new = pd.get_dummies(merged_df[features], drop_first=True)\n",
    "\n",
    "# Обработка данных: замена бесконечных значений и NaN\n",
    "X_new.replace([float('inf'), float('-inf')], pd.NA, inplace=True)\n",
    "X_new.dropna(inplace=True)\n",
    "\n",
    "# Сохранение индексов оставшихся строк\n",
    "valid_indices = X_new.index\n",
    "\n",
    "# Обеспечение соответствия признаков модели\n",
    "X_new = X_new.reindex(columns=model.feature_names_in_, fill_value=0)\n",
    "\n",
    "# Выполнение предсказаний\n",
    "predictions = model.predict(X_new)\n",
    "\n",
    "# Создание нового столбца с предсказанными значениями только для действительных индексов\n",
    "merged_df.loc[valid_indices, 'predicted_popularity'] = predictions\n",
    "\n",
    "# Удаление дубликатов по video_id\n",
    "unique_videos = merged_df.drop_duplicates(subset='video_id')\n",
    "\n",
    "# Сортировка по предсказанной популярности\n",
    "unique_videos = unique_videos.sort_values(by='predicted_popularity', ascending=False)\n",
    "\n",
    "# Поиск максимальных 3 значений percent с тегом\n",
    "top_tags = tags_df.nlargest(3, 'percent')\n",
    "\n",
    "# Список для хранения финальных видео\n",
    "final_video_ids = []\n",
    "\n",
    "# Выбор случайного числа для изменения выборки\n",
    "random.seed(time.time())\n",
    "\n",
    "# Поиск двух популярных видео для каждого из топовых тегов\n",
    "for tag in top_tags['tag']:\n",
    "    # Находим все видео с данным тегом\n",
    "    videos_with_tag = unique_videos[unique_videos['category_id'].str.contains(tag, na=False)]\n",
    "    \n",
    "    if not videos_with_tag.empty:\n",
    "        # Выбор до 2 видео с этим тегом\n",
    "        selected_videos = videos_with_tag.sample(n=min(2, len(videos_with_tag)), random_state=random.randint(0, 100))\n",
    "        final_video_ids.extend(selected_videos['video_id'].tolist())\n",
    "\n",
    "# Если всё ещё не достигли 10 видео, добавляем случайные видео из других категорий\n",
    "remaining_slots = 10 - len(final_video_ids)\n",
    "\n",
    "if remaining_slots > 0:\n",
    "    popular_others = unique_videos[~unique_videos['video_id'].isin(final_video_ids)].sample(\n",
    "        n=min(remaining_slots, len(unique_videos[~unique_videos['video_id'].isin(final_video_ids)])),\n",
    "              random_state=random.randint(0, 100))\n",
    "        \n",
    "    final_video_ids.extend(popular_others['video_id'].tolist())\n",
    "\n",
    "# Удаляем дубликаты, если они есть\n",
    "final_video_ids = list(set(final_video_ids))\n",
    "\n",
    "# Получаем финальные 10 видео\n",
    "final_video_ids = final_video_ids[:10]\n",
    "\n",
    "# Проверка на наличие файла security.parquet и его содержимого\n",
    "try:\n",
    "    security_df = pd.read_parquet('security.parquet')\n",
    "except ValueError:\n",
    "    # Если файл не существует или пуст, то создаем пустая DataFrame\n",
    "    security_df = pd.DataFrame(columns=['video_id'])\n",
    "\n",
    "if security_df.empty:\n",
    "    # Если файл пуст, просто используем финальные video_id\n",
    "    existing_video_ids = set()  # Пустое множество, так как ничего нет в security.parquet\n",
    "else:\n",
    "    existing_video_ids = set(security_df['video_id'])\n",
    "\n",
    "# Генерация новых video_id в случае совпадений\n",
    "while any(video_id in existing_video_ids for video_id in final_video_ids):\n",
    "    # Ищем альтернативные video_id, пока все не будут уникальными\n",
    "    additional_videos = unique_videos[~unique_videos['video_id'].isin(final_video_ids)].sample(\n",
    "    n=len(final_video_ids), random_state=random.randint(0, 100))\n",
    "    \n",
    "    final_video_ids = additional_videos['video_id'].tolist()\n",
    "    final_video_ids = list(set(final_video_ids))[:10]  # Убедимся, что оставим только 10\n",
    "\n",
    "# Получаем данные о финальных видео\n",
    "final_videos = unique_videos[unique_videos['video_id'].isin(final_video_ids)]\n",
    "\n",
    "# Объединяем с дополнительным DataFrame для получения title и v_pub_datetime\n",
    "final_videos_info = pd.merge(final_videos, additional_df[['video_id', 'title', 'v_pub_datetime']],\n",
    "                              on='video_id', how='left')\n",
    "\n",
    "# Сохранение итоговых video_id в файл security.parquet\n",
    "final_video_ids_df = pd.DataFrame(final_video_ids, columns=['video_id'])\n",
    "\n",
    "# Добавляем новые video_id в security.parquet, если они уникальны\n",
    "new_ids_to_add = final_video_ids_df[~final_video_ids_df['video_id'].isin(existing_video_ids)]\n",
    "\n",
    "if not new_ids_to_add.empty:\n",
    "    # Объединяем с существующими данными\n",
    "    updated_security_df = pd.concat([security_df, new_ids_to_add], ignore_index=True)\n",
    "    updated_security_df.to_parquet('security.parquet', index=False)\n",
    "\n",
    "# Вывод финальных видео\n",
    "print(final_videos_info[['video_id', 'title', 'v_pub_datetime', 'category_id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c458c7a-7be5-4c80-9f2b-013592e99572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                video_id\n",
      "0   9f74dc31-255f-448c-8f52-4d0c3c0100da\n",
      "1   7523cdb0-e050-46f3-a8b8-b51454e9ed59\n",
      "2   0119fbc7-db81-4ef1-99d5-55600348d9d0\n",
      "3   53d7fd98-ba32-4913-86a9-11ee1f01948f\n",
      "4   6230fb26-948b-45cc-8df7-526c644db075\n",
      "..                                   ...\n",
      "85  b3181c61-2d66-4114-b0ad-e891c08e20c5\n",
      "86  20618a8d-3bd9-4a30-8b32-eabf3681b7fb\n",
      "87  6355965d-ae67-419a-b8c8-d088d1eaecdb\n",
      "88  e5fa6850-a969-4fc9-ae57-8f542b8fd196\n",
      "89  4e972100-6a11-4dc2-9146-bb955526af46\n",
      "\n",
      "[90 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "#вывод уже использованных\n",
    "import pandas as pd\n",
    "\n",
    "# Чтение Parquet файла\n",
    "df = pd.read_parquet('security.parquet')\n",
    "\n",
    "# Вывод первых 5 строк\n",
    "print(df.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab3cef4-9133-4699-b52c-9f1e10a2c041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cf9048e-f6eb-410a-9d08-d0b51b66a26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Итоговая таблица сохранена в final_videos_info.json.\n",
      "                                    uid   category_id  v_likes  v_dislikes  \\\n",
      "0  33231c5e-8ade-4361-b488-c8575287a221       Сериалы       94           1   \n",
      "1  99dc8571-3206-4686-85e8-fe1ea6da4436   Развлечения       23           0   \n",
      "2  e50f5c7a-a4e2-419b-8a13-18ea219b599a  Телепередачи       19           0   \n",
      "3  3d2895f8-d928-4971-81af-74adaf2288b4   Мультфильмы      668          28   \n",
      "4  30e52a56-9d48-498b-ac68-617868a59b29   Развлечения      179           4   \n",
      "5  a9672629-8ea1-4349-8091-e1dfc0c411d4    Аудиокниги        8           0   \n",
      "6  f1d6389b-9a11-4bcf-adaf-8aa2ada24c61         Детям       91           3   \n",
      "7  58aac622-479c-4011-ae6d-1001b52a9a50   Мультфильмы        6           0   \n",
      "8  73ed63d5-42ce-4eed-a7bd-5fb2eb37a752   Путешествия        2           0   \n",
      "9  3f96ca41-00b6-450d-9aae-23d9c13f628b   Развлечения        0           0   \n",
      "\n",
      "   v_frac_avg_watchtime_1_day_duration  v_frac_avg_watchtime_7_day_duration  \\\n",
      "0                             0.572570                             0.565483   \n",
      "1                             0.393983                             0.485252   \n",
      "2                             0.441108                             0.348330   \n",
      "3                             0.299447                             0.318605   \n",
      "4                             0.291638                             0.307543   \n",
      "5                             0.351397                             0.296959   \n",
      "6                             0.679754                             0.668458   \n",
      "7                             0.629487                             0.746999   \n",
      "8                             0.054903                             0.067777   \n",
      "9                             1.021982                             1.021982   \n",
      "\n",
      "   v_frac_avg_watchtime_30_day_duration                                region  \\\n",
      "0                              0.565364  f5925ad5-7bc3-4c10-b14b-807f0cdd8c4e   \n",
      "1                              0.530442  f5925ad5-7bc3-4c10-b14b-807f0cdd8c4e   \n",
      "2                              0.263237  532446db-2a74-4273-b810-b4fd84f6d259   \n",
      "3                              0.310708  f5925ad5-7bc3-4c10-b14b-807f0cdd8c4e   \n",
      "4                              0.303448  85e1979a-7fa9-4e7e-871f-2ff110292295   \n",
      "5                              0.299539  4ea90a54-312b-43bf-b84b-7573be1ea2ce   \n",
      "6                              0.645514  1b89e709-f17e-445d-b456-a1edef65f2c4   \n",
      "7                              0.734932  aa3fa1f7-75ad-4acb-9c67-1f06b9ac0b2a   \n",
      "8                              0.055934  aa3fa1f7-75ad-4acb-9c67-1f06b9ac0b2a   \n",
      "9                              1.021982  f5925ad5-7bc3-4c10-b14b-807f0cdd8c4e   \n",
      "\n",
      "                                   city  watchtime  predicted_popularity  \n",
      "0  c154d199-d883-4e3c-81ee-fab730e7905f        811          16408.373686  \n",
      "1  c154d199-d883-4e3c-81ee-fab730e7905f        521           7560.769462  \n",
      "2  7c616cd4-7004-4a8e-b20d-4130ca1d6a37        151           6230.985205  \n",
      "3  c154d199-d883-4e3c-81ee-fab730e7905f         22           5833.019457  \n",
      "4  9002b3b1-b5bc-4b9c-9d42-d5ba0d757fdf        160           5483.307438  \n",
      "5  446e38d7-da89-4ade-8319-da62dc8afd96       2898           3886.301141  \n",
      "6  5d93815d-cefe-41b0-b66b-05f6c4c066d4        301           2360.323192  \n",
      "7  1263bd24-a853-471c-a4f0-7266cc829c2f        388           1539.323966  \n",
      "8  1263bd24-a853-471c-a4f0-7266cc829c2f         22            865.338045  \n",
      "9  c154d199-d883-4e3c-81ee-fab730e7905f        150            751.083307  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Загрузка модели\n",
    "model = joblib.load('random_forest_model.joblib')\n",
    "\n",
    "# Загрузка данных из файлов\n",
    "filtred_df = pd.read_parquet('tags-filtered.parquet')\n",
    "logs_df = pd.read_parquet('logsV02.parquet')\n",
    "tags_df = pd.read_parquet('users/user0_tags.parquet')\n",
    "regions_df = pd.read_parquet('users/user0_regions.parquet')\n",
    "city_df = pd.read_parquet('users/user0_city.parquet')\n",
    "additional_df = pd.read_parquet('parquet-filtered/filtred.parquet')\n",
    "\n",
    "# Объединение данных по video_id\n",
    "merged_df = pd.merge(filtred_df, logs_df, on='video_id', how='inner')\n",
    "\n",
    "# Ограничиваемся первыми 1000 записями\n",
    "merged_df = merged_df.head(1000)\n",
    "\n",
    "# Признаки\n",
    "features = ['category_id', 'v_likes', 'v_dislikes',\n",
    "            'v_frac_avg_watchtime_1_day_duration',\n",
    "            'v_frac_avg_watchtime_7_day_duration',\n",
    "            'v_frac_avg_watchtime_30_day_duration']\n",
    "\n",
    "# Преобразование категориальных переменных для новых данных\n",
    "X_new = pd.get_dummies(merged_df[features], drop_first=True)\n",
    "\n",
    "# Обработка данных: замена бесконечных значений и NaN\n",
    "X_new.replace([float('inf'), float('-inf')], pd.NA, inplace=True)\n",
    "X_new.dropna(inplace=True)\n",
    "\n",
    "# Сохранение индексов оставшихся строк\n",
    "valid_indices = X_new.index\n",
    "\n",
    "# Обеспечение соответствия признаков модели\n",
    "X_new = X_new.reindex(columns=model.feature_names_in_, fill_value=0)\n",
    "\n",
    "# Выполнение предсказаний\n",
    "predictions = model.predict(X_new)\n",
    "\n",
    "# Создание нового столбца с предсказанными значениями только для действительных индексов\n",
    "merged_df.loc[valid_indices, 'predicted_popularity'] = predictions\n",
    "\n",
    "# Удаление дубликатов по video_id\n",
    "unique_videos = merged_df.drop_duplicates(subset='video_id')\n",
    "\n",
    "# Сортировка по предсказанной популярности\n",
    "unique_videos = unique_videos.sort_values(by='predicted_popularity', ascending=False)\n",
    "\n",
    "# Поиск максимальных 3 значений percent с тегом\n",
    "top_tags = tags_df.nlargest(3, 'percent')\n",
    "\n",
    "# Список для хранения финальных видео\n",
    "final_video_ids = []\n",
    "\n",
    "# Выбор случайного числа для изменения выборки\n",
    "random.seed(time.time())\n",
    "\n",
    "# Поиск двух популярных видео для каждого из топовых тегов\n",
    "for tag in top_tags['tag']:\n",
    "    videos_with_tag = unique_videos[unique_videos['category_id'].str.contains(tag, na=False)]\n",
    "    \n",
    "    if not videos_with_tag.empty:\n",
    "        selected_videos = videos_with_tag.sample(n=min(2, len(videos_with_tag)), random_state=random.randint(0, 100))\n",
    "        final_video_ids.extend(selected_videos['video_id'].tolist())\n",
    "\n",
    "# Если всё ещё не достигли 10 видео, добавляем случайные видео из других категорий\n",
    "remaining_slots = 10 - len(final_video_ids)\n",
    "\n",
    "if remaining_slots > 0:\n",
    "    popular_others = unique_videos[~unique_videos['video_id'].isin(final_video_ids)].sample(\n",
    "        n=min(remaining_slots, len(unique_videos[~unique_videos['video_id'].isin(final_video_ids)])),\n",
    "              random_state=random.randint(0, 100))\n",
    "        \n",
    "    final_video_ids.extend(popular_others['video_id'].tolist())\n",
    "\n",
    "# Удаляем дубликаты, если они есть\n",
    "final_video_ids = list(set(final_video_ids))\n",
    "\n",
    "# Получаем финальные 10 видео\n",
    "final_video_ids = final_video_ids[:10]\n",
    "\n",
    "# Проверка на наличие файла security.parquet и его содержимого\n",
    "try:\n",
    "    security_df = pd.read_parquet('security.parquet')\n",
    "except ValueError:\n",
    "    # Если файл не существует или пуст, то создаем пустая DataFrame\n",
    "    security_df = pd.DataFrame(columns=['video_id'])\n",
    "\n",
    "if security_df.empty:\n",
    "    existing_video_ids = set()  # Пустое множество, так как ничего нет в security.parquet\n",
    "else:\n",
    "    existing_video_ids = set(security_df['video_id'])\n",
    "\n",
    "# Генерация новых video_id в случае совпадений\n",
    "while any(video_id in existing_video_ids for video_id in final_video_ids):\n",
    "    final_video_ids = []\n",
    "    \n",
    "    for tag in top_tags['tag']:\n",
    "        videos_with_tag = unique_videos[unique_videos['category_id'].str.contains(tag, na=False)]\n",
    "        \n",
    "        if not videos_with_tag.empty:\n",
    "            selected_videos = videos_with_tag.sample(n=min(2, len(videos_with_tag)), random_state=random.randint(0, 100))\n",
    "            final_video_ids.extend(selected_videos['video_id'].tolist())\n",
    "            remaining_slots = 10 - len(final_video_ids)\n",
    "\n",
    "    if remaining_slots > 0:\n",
    "        popular_others = unique_videos[~unique_videos['video_id'].isin(final_video_ids)].sample(\n",
    "            n=min(remaining_slots, len(unique_videos[~unique_videos['video_id'].isin(final_video_ids)])),\n",
    "                  random_state=random.randint(0, 100))\n",
    "\n",
    "        final_video_ids.extend(popular_others['video_id'].tolist())\n",
    "\n",
    "    final_video_ids = list(set(final_video_ids))  # Удаляем дубликаты\n",
    "    final_video_ids = final_video_ids[:10]  # Оставляем только 10 видео\n",
    "\n",
    "# Создание итогового DataFrame с информацией о видео\n",
    "final_videos_info = unique_videos[unique_videos['video_id'].isin(final_video_ids)].copy()\n",
    "\n",
    "# Переименование столбца video_id в uid\n",
    "final_videos_info.rename(columns={'video_id': 'uid'}, inplace=True)\n",
    "\n",
    "# Сохранение итогового DataFrame в файл JSON\n",
    "final_videos_info.to_json('final_videos_info.json', orient='records', lines=True)\n",
    "\n",
    "print(\"Итоговая таблица сохранена в final_videos_info.json.\")\n",
    "\n",
    "# Загрузка DataFrame из JSON для проверки\n",
    "df = pd.read_json('final_videos_info.json', lines=True)\n",
    "\n",
    "# Вывод первых 100 строк\n",
    "print(df.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0203572-20d9-467a-a5b7-df4d18396e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4872ce-9f05-4226-a100-4f1247b1b75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Загрузка модели\n",
    "model = joblib.load('random_forest_model.joblib')\n",
    "\n",
    "# Загрузка данных из файлов\n",
    "filtred_df = pd.read_parquet('tags-filtered.parquet')\n",
    "logs_df = pd.read_parquet('logsV02.parquet')\n",
    "tags_df = pd.read_parquet('users/user0_tags.parquet')\n",
    "regions_df = pd.read_parquet('users/user0_regions.parquet')\n",
    "city_df = pd.read_parquet('users/user0_city.parquet')\n",
    "additional_df = pd.read_parquet('parquet-filtered/filtred.parquet')\n",
    "\n",
    "# Объединение данных по video_id\n",
    "merged_df = pd.merge(filtred_df, logs_df, on='video_id', how='inner')\n",
    "\n",
    "# Ограничиваемся первыми 1000 записями\n",
    "merged_df = merged_df.head(1000)\n",
    "\n",
    "# Признаки\n",
    "features = ['category_id', 'v_likes', 'v_dislikes',\n",
    "            'v_frac_avg_watchtime_1_day_duration',\n",
    "            'v_frac_avg_watchtime_7_day_duration',\n",
    "            'v_frac_avg_watchtime_30_day_duration']\n",
    "\n",
    "# Преобразование категориальных переменных для новых данных\n",
    "X_new = pd.get_dummies(merged_df[features], drop_first=True)\n",
    "\n",
    "# Обработка данных: замена бесконечных значений и NaN\n",
    "X_new.replace([float('inf'), float('-inf')], pd.NA, inplace=True)\n",
    "X_new.dropna(inplace=True)\n",
    "\n",
    "# Сохранение индексов оставшихся строк\n",
    "valid_indices = X_new.index\n",
    "\n",
    "# Обеспечение соответствия признаков модели\n",
    "X_new = X_new.reindex(columns=model.feature_names_in_, fill_value=0)\n",
    "\n",
    "# Выполнение предсказаний\n",
    "predictions = model.predict(X_new)\n",
    "\n",
    "# Создание нового столбца с предсказанными значениями только для действительных индексов\n",
    "merged_df.loc[valid_indices, 'predicted_popularity'] = predictions\n",
    "\n",
    "# Удаление дубликатов по video_id\n",
    "unique_videos = merged_df.drop_duplicates(subset='video_id')\n",
    "\n",
    "# Сортировка по предсказанной популярности\n",
    "unique_videos = unique_videos.sort_values(by='predicted_popularity', ascending=False)\n",
    "\n",
    "# Поиск максимальных 3 значений percent с тегом\n",
    "top_tags = tags_df.nlargest(3, 'percent')\n",
    "\n",
    "# Список для хранения финальных видео\n",
    "final_video_ids = []\n",
    "\n",
    "# Выбор случайного числа для изменения выборки\n",
    "random.seed(time.time())\n",
    "\n",
    "# Поиск двух популярных видео для каждого из топовых тегов\n",
    "for tag in top_tags['tag']:\n",
    "    # Находим все видео с данным тегом\n",
    "    videos_with_tag = unique_videos[unique_videos['category_id'].str.contains(tag, na=False)]\n",
    "    \n",
    "    if not videos_with_tag.empty:\n",
    "        # Выбор до 2 видео с этим тегом\n",
    "        selected_videos = videos_with_tag.sample(n=min(2, len(videos_with_tag)), random_state=random.randint(0, 100))\n",
    "        final_video_ids.extend(selected_videos['video_id'].tolist())\n",
    "\n",
    "# Добавление топ-3 видео по регионам\n",
    "top_regions = regions_df.nlargest(3, 'region').video_id.tolist()\n",
    "final_video_ids.extend(top_regions)\n",
    "\n",
    "# Добавление топ-3 видео по городам\n",
    "top_cities = city_df.nlargest(3, 'city').video_id.tolist()\n",
    "final_video_ids.extend(top_cities)\n",
    "\n",
    "# Если всё ещё не достигли 10 видео, добавляем случайные видео из других категорий\n",
    "remaining_slots = 10 - len(final_video_ids)\n",
    "\n",
    "if remaining_slots > 0:\n",
    "    popular_others = unique_videos[~unique_videos['video_id'].isin(final_video_ids)].sample(\n",
    "        n=min(remaining_slots, len(unique_videos[~unique_videos['video_id'].isin(final_video_ids)])),\n",
    "              random_state=random.randint(0, 100))\n",
    "        \n",
    "    final_video_ids.extend(popular_others['video_id'].tolist())\n",
    "\n",
    "# Удаляем дубликаты, если они есть\n",
    "final_video_ids = list(set(final_video_ids))\n",
    "\n",
    "# Получаем финальные 10 видео\n",
    "final_video_ids = final_video_ids[:10]\n",
    "\n",
    "# Проверка на наличие файла security.parquet и его содержимого\n",
    "try:\n",
    "    security_df = pd.read_parquet('security.parquet')\n",
    "except ValueError:\n",
    "    # Если файл не существует или пуст, то создаем пустая DataFrame\n",
    "    security_df = pd.DataFrame(columns=['video_id'])\n",
    "\n",
    "if security_df.empty:\n",
    "    # Если файл пуст, просто используем финальные video_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7106303d-4255-431d-b334-832468bfdfd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a4966e8-df3b-47c7-adcb-64eafbc154ce",
   "metadata": {},
   "source": [
    "## С учётом региона и города"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc43945-b45e-4679-985c-ddeadb2c5fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Загрузка модели\n",
    "model = joblib.load('random_forest_model.joblib')\n",
    "\n",
    "# Загрузка данных из файлов\n",
    "filtred_df = pd.read_parquet('tags-filtered.parquet')\n",
    "logs_df = pd.read_parquet('logsV02.parquet')\n",
    "tags_df = pd.read_parquet('users/user0_tags.parquet')\n",
    "regions_df = pd.read_parquet('users/user0_regions.parquet')\n",
    "city_df = pd.read_parquet('users/user0_city.parquet')\n",
    "additional_df = pd.read_parquet('parquet-filtered/filtred.parquet')\n",
    "\n",
    "# Объединение данных по video_id\n",
    "merged_df = pd.merge(filtred_df, logs_df, on='video_id', how='inner')\n",
    "\n",
    "# Ограничиваемся первыми 1000 записями\n",
    "merged_df = merged_df.head(1000)\n",
    "\n",
    "# Признаки\n",
    "features = ['category_id', 'v_likes', 'v_dislikes',\n",
    "            'v_frac_avg_watchtime_1_day_duration',\n",
    "            'v_frac_avg_watchtime_7_day_duration',\n",
    "            'v_frac_avg_watchtime_30_day_duration']\n",
    "\n",
    "# Преобразование категориальных переменных для новых данных\n",
    "X_new = pd.get_dummies(merged_df[features], drop_first=True)\n",
    "\n",
    "# Обработка данных: замена бесконечных значений и NaN\n",
    "X_new.replace([float('inf'), float('-inf')], pd.NA, inplace=True)\n",
    "X_new.dropna(inplace=True)\n",
    "\n",
    "# Сохранение индексов оставшихся строк\n",
    "valid_indices = X_new.index\n",
    "\n",
    "# Обеспечение соответствия признаков модели\n",
    "X_new = X_new.reindex(columns=model.feature_names_in_, fill_value=0)\n",
    "\n",
    "# Выполнение предсказаний\n",
    "predictions = model.predict(X_new)\n",
    "\n",
    "# Создание нового столбца с предсказанными значениями только для действительных индексов\n",
    "merged_df.loc[valid_indices, 'predicted_popularity'] = predictions\n",
    "\n",
    "# Удаление дубликатов по video_id\n",
    "unique_videos = merged_df.drop_duplicates(subset='video_id')\n",
    "\n",
    "# Сортировка по предсказанной популярности\n",
    "unique_videos = unique_videos.sort_values(by='predicted_popularity', ascending=False)\n",
    "\n",
    "# Поиск максимальных 3 значений percent с тегом\n",
    "top_tags = tags_df.nlargest(3, 'percent')\n",
    "\n",
    "# Список для хранения финальных видео\n",
    "final_video_ids = []\n",
    "\n",
    "# Выбор случайного числа для изменения выборки\n",
    "random.seed(time.time())\n",
    "\n",
    "# Поиск двух популярных видео для каждого из топовых тегов\n",
    "for tag in top_tags['tag']:\n",
    "    # Находим все видео с данным тегом\n",
    "    videos_with_tag = unique_videos[unique_videos['category_id'].str.contains(tag, na=False)]\n",
    "    \n",
    "    if not videos_with_tag.empty:\n",
    "        # Выбор до 2 видео с этим тегом\n",
    "        selected_videos = videos_with_tag.sample(n=min(2, len(videos_with_tag)), random_state=random.randint(0, 100))\n",
    "        final_video_ids.extend(selected_videos['video_id'].tolist())\n",
    "\n",
    "# Добавление топ-3 видео по регионам\n",
    "top_regions = regions_df.nlargest(3, 'region').video_id.tolist()\n",
    "final_video_ids.extend(top_regions)\n",
    "\n",
    "# Добавление топ-3 видео по городам\n",
    "top_cities = city_df.nlargest(3, 'city').video_id.tolist()\n",
    "final_video_ids.extend(top_cities)\n",
    "\n",
    "# Если всё ещё не достигли 10 видео, добавляем случайные видео из других категорий\n",
    "remaining_slots = 10 - len(final_video_ids)\n",
    "\n",
    "if remaining_slots > 0:\n",
    "    popular_others = unique_videos[~unique_videos['video_id'].isin(final_video_ids)].sample(\n",
    "        n=min(remaining_slots, len(unique_videos[~unique_videos['video_id'].isin(final_video_ids)])),\n",
    "              random_state=random.randint(0, 100))\n",
    "        \n",
    "    final_video_ids.extend(popular_others['video_id'].tolist())\n",
    "\n",
    "# Удаляем дубликаты, если они есть\n",
    "final_video_ids = list(set(final_video_ids))\n",
    "\n",
    "# Получаем финальные 10 видео\n",
    "final_video_ids = final_video_ids[:10]\n",
    "\n",
    "# Проверка на наличие файла security.parquet и его содержимого\n",
    "try:\n",
    "    security_df = pd.read_parquet('security.parquet')\n",
    "except ValueError:\n",
    "    # Если файл не существует или пуст, то создаем пустая DataFrame\n",
    "    security_df = pd.DataFrame(columns=['video_id'])\n",
    "\n",
    "if security_df.empty:\n",
    "    # Если файл пуст, просто используем финальные video_id\n",
    "    pass\n",
    "else:\n",
    "    # Если файл не пустой, проверяем наличие video_id из финального списка\n",
    "    existing_video_ids = security_df['video_id'].unique().tolist()\n",
    "    \n",
    "    # Фильтруем финальные video_id, исключая уже существующие\n",
    "    final_video_ids = [vid for vid in final_video_ids if vid not in existing_video_ids]\n",
    "\n",
    "# Проверяем, есть ли еще места до 10 видео\n",
    "remaining_slots = 10 - len(final_video_ids)\n",
    "\n",
    "if remaining_slots > 0:\n",
    "    # Получаем дополнительные видео из уникальных видео, которые ещё не в финальном списке\n",
    "    additional_videos = unique_videos[~unique_videos['video_id'].isin(final_video_ids)]\n",
    "    \n",
    "    if not additional_videos.empty:\n",
    "        # Выбираем случайные видео для заполнения оставшихся слотов\n",
    "        more_videos = additional_videos.sample(n=min(remaining_slots, len(additional_videos)),\n",
    "                                               random_state=random.randint(0, 100))\n",
    "        final_video_ids.extend(more_videos['video_id'].tolist())\n",
    "\n",
    "# Удаляем дубликаты и ограничиваем длину списка до 10\n",
    "final_video_ids = list(set(final_video_ids))[:10]\n",
    "\n",
    "# Печатаем или сохраняем финальные video_id\n",
    "print(\"Финальные рекомендованные video_id:\", final_video_ids)\n",
    "\n",
    "# Здесь можно добавить код для сохранения рекомендаций, например, в новый parquet файл\n",
    "final_recommendations_df = pd.DataFrame({'video_id': final_video_ids})\n",
    "final_recommendations_df.to_parquet('final_recommendations.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024c0ba1-1402-418f-979c-7a450b7dd20e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba08df24-4f35-4c26-8002-4b1e6049b1fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
